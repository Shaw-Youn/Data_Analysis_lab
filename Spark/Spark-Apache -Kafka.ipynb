{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "353819d1",
   "metadata": {},
   "source": [
    "### A. Starting Kafka and Zookeeper\n",
    "\n",
    "If you haven't already started Kafka and Zookeeper, you need to do so:\n",
    "\n",
    "1. **Start Zookeeper**:\n",
    "   ```sh\n",
    "   bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "   ```\n",
    "\n",
    "2. **Start Kafka**:\n",
    "   ```sh\n",
    "   bin/kafka-server-start.sh config/server.properties\n",
    "   ```\n",
    "3. **Create a Topic**:\n",
    "   ```sh\n",
    "   bin/kafka-topics.sh --create --topic input-events --bootstrap-server localhost:9092\n",
    "   ```\n",
    "\n",
    "Great! You've successfully created a Kafka topic named `quick-events`. The next steps typically involve producing some messages to this topic and then consuming those messages. Here are the commands for both:\n",
    "\n",
    "### Producing Messages to the Topic\n",
    "\n",
    "1. **Start a Kafka Producer**:\n",
    "   ```sh\n",
    "   bin/kafka-console-producer.sh --topic quick-events --bootstrap-server localhost:9092\n",
    "   ```\n",
    "   This command will open a prompt where you can start typing messages. Each line you type will be sent as a message to the `quick-events` topic. To exit, press `Ctrl+C`.\n",
    "\n",
    "### Consuming Messages from the Topic\n",
    "\n",
    "2. **Start a Kafka Consumer**:\n",
    "   ```sh\n",
    "   bin/kafka-console-consumer.sh --topic quick-events --from-beginning --bootstrap-server localhost:9092\n",
    "   ```\n",
    "   This command will start a consumer that reads messages from the `quick-events` topic starting from the beginning.\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "These commands should be run in separate terminal windows to keep Zookeeper and Kafka running.\n",
    "\n",
    "### Summary of Commands\n",
    "\n",
    "\n",
    "1. **Start Zookeeper (if not already running)**:\n",
    "   ```sh\n",
    "   bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "   ```\n",
    "\n",
    "2. **Start Kafka (if not already running)**:\n",
    "   ```sh\n",
    "   bin/kafka-server-start.sh config/server.properties\n",
    "   ```\n",
    "\n",
    "3. **Create a Topic**:\n",
    "   ```sh\n",
    "   bin/kafka-topics.sh --create --topic quick-events --bootstrap-server localhost:9092\n",
    "   ```\n",
    "\n",
    "4. **Start a Producer**:\n",
    "   ```sh\n",
    "   bin/kafka-console-producer.sh --topic quick-events --bootstrap-server localhost:9092\n",
    "   ```\n",
    "\n",
    "5. **Start a Consumer**:\n",
    "   ```sh\n",
    "   bin/kafka-console-consumer.sh --topic quick-events --from-beginning --bootstrap-server localhost:9092\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f93e4",
   "metadata": {},
   "source": [
    "To set the `PYSPARK_SUBMIT_ARGS` environment variable with the appropriate Kafka package for Spark 3.5.1 and Scala 2.12.18, and Kafka 2.13-3.7.1, you need to ensure that the Kafka connector matches your Spark and Scala versions. Since your Spark version is 3.5.1 and it uses Scala 2.12, you need to use the Kafka connector for Scala 2.12. \n",
    "\n",
    "Here’s how to set it up:\n",
    "\n",
    "### B. Setting Up Your Jupyter Notebook\n",
    "\n",
    "\n",
    "1. **Create a Spark Session**:\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "\n",
    "   spark = SparkSession.builder \\\n",
    "       .appName(\"KafkaExample\") \\\n",
    "       .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\n",
    "       .getOrCreate()\n",
    "   ```\n",
    "\n",
    "2. **Reading from Kafka**:\n",
    "   ```python\n",
    "   kafka_df = spark.readStream \\\n",
    "       .format(\"kafka\") \\\n",
    "       .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "       .option(\"subscribe\", \"quick-events\") \\\n",
    "       .load()\n",
    "\n",
    "   kafka_df = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "   ```\n",
    "\n",
    "3. **Writing to Kafka**:\n",
    "   ```python\n",
    "   data = [(\"key1\", \"message1\"), (\"key2\", \"message2\")]\n",
    "   columns = [\"key\", \"value\"]\n",
    "   df = spark.createDataFrame(data, columns)\n",
    "   kafka_df_write = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "   kafka_df_write.write \\\n",
    "       .format(\"kafka\") \\\n",
    "       .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "       .option(\"topic\", \"quick-events\") \\\n",
    "       .save()\n",
    "   ```\n",
    "\n",
    "### Full Example Notebook\n",
    "\n",
    "Here’s the complete example in a Jupyter Notebook cell, ensuring compatibility:\n",
    "\n",
    "```python\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaExample\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark version\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Reading from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"quick-events\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Write to Console (streaming)\n",
    "query = kafka_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Example data to write to Kafka\n",
    "data = [(\"key1\", \"message1\"), (\"key2\", \"message2\")]\n",
    "columns = [\"key\", \"value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "kafka_df_write = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Write to Kafka\n",
    "kafka_df_write.write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"quick-events\") \\\n",
    "    .save()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **Ensure Kafka Server is Running**: Make sure your Kafka server is up and running at `localhost:9092`.\n",
    "- **Version Compatibility**: Double-check that the versions of Spark, Scala, and the Kafka connector are compatible. The key here is to match the Scala version used by Spark.\n",
    "- **Network Access**: Ensure your environment has network access to download dependencies when using the `--packages` option.\n",
    "\n",
    "By including the correct Kafka package and Scala library for your Spark and Scala versions, the missing class definition error should be resolved. If you continue to encounter issues, please provide further details for additional troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c9439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
