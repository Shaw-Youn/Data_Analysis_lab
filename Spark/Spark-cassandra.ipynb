{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e6fb76",
   "metadata": {},
   "source": [
    "**(A)**. To read from and write to Cassandra using PySpark, you need to set up the necessary configurations and dependencies. Below are the steps to do so, along with example code snippets.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. **Cassandra Cluster**: Ensure you have a running Cassandra cluster.\n",
    "2. **PySpark**: Make sure PySpark is installed.\n",
    "3. **Cassandra Connector**: Use the `spark-cassandra-connector` to connect PySpark with Cassandra.\n",
    "\n",
    "### Step-by-Step Guide\n",
    "\n",
    "#### 1. Set up Dependencies\n",
    "\n",
    "First, ensure you have the `spark-cassandra-connector` package. If you are using a notebook or a local PySpark setup, you can specify the package when initializing the Spark session.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraSparkIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"your_cassandra_host\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "Replace `\"your_cassandra_host\"` with the IP address or hostname of your Cassandra node.\n",
    "\n",
    "#### 2. Reading from Cassandra\n",
    "\n",
    "To read data from a Cassandra table, you need to specify the keyspace and table name.\n",
    "\n",
    "```python\n",
    "# Read data from Cassandra\n",
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"your_table_name\", keyspace=\"your_keyspace_name\") \\\n",
    "    .load()\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "```\n",
    "\n",
    "Replace `\"your_table_name\"` and `\"your_keyspace_name\"` with your Cassandra table and keyspace names.\n",
    "\n",
    "#### 3. Writing to Cassandra\n",
    "\n",
    "To write data to a Cassandra table, you need to create a DataFrame and use the `.write` method.\n",
    "\n",
    "```python\n",
    "# Example DataFrame to write\n",
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Smith\", 25)]\n",
    "columns = [\"first_name\", \"last_name\", \"age\"]\n",
    "df_to_write = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write DataFrame to Cassandra\n",
    "df_to_write.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"your_table_name\", keyspace=\"your_keyspace_name\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "```\n",
    "\n",
    "Again, replace `\"your_table_name\"` and `\"your_keyspace_name\"` with your Cassandra table and keyspace names.\n",
    "\n",
    "### Complete Example\n",
    "\n",
    "Here is a complete example that combines reading from and writing to Cassandra.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraSparkIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"your_cassandra_host\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from Cassandra\n",
    "df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"your_table_name\", keyspace=\"your_keyspace_name\") \\\n",
    "    .load()\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Example DataFrame to write\n",
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Smith\", 25)]\n",
    "columns = [\"first_name\", \"last_name\", \"age\"]\n",
    "df_to_write = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write DataFrame to Cassandra\n",
    "df_to_write.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"your_table_name\", keyspace=\"your_keyspace_name\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "1. **Dependencies**: Make sure the `spark-cassandra-connector` is compatible with your version of Spark and Scala.\n",
    "2. **Cassandra Configurations**: You can add additional configurations like authentication details if your Cassandra cluster requires it.\n",
    "3. **Error Handling**: Add appropriate error handling for production code.\n",
    "\n",
    "This should help you get started with reading from and writing to Cassandra using PySpark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b0b07",
   "metadata": {},
   "source": [
    "**(B)**. To read stream data from a source and write it to Cassandra using PySpark, you need to integrate Spark Structured Streaming with the Cassandra connector. Here’s a step-by-step guide to help you achieve this.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. **Cassandra Cluster**: Ensure you have a running Cassandra cluster.\n",
    "2. **PySpark**: Make sure PySpark is installed.\n",
    "3. **Cassandra Connector**: Use the `spark-cassandra-connector` to connect PySpark with Cassandra.\n",
    "4. **Streaming Source**: Have a streaming source like Kafka, socket, or a file source set up.\n",
    "\n",
    "### Step-by-Step Guide\n",
    "\n",
    "#### 1. Set up Dependencies\n",
    "\n",
    "Make sure you have the `spark-cassandra-connector` package. If you are using a notebook or a local PySpark setup, specify the package when initializing the Spark session.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraSparkStreamingIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "Replace `\"localhost\"` with the IP address or hostname of your Cassandra node if it's not running locally.\n",
    "\n",
    "#### 2. Read Streaming Data\n",
    "\n",
    "For this example, let’s assume you are reading streaming data from Kafka.\n",
    "\n",
    "```python\n",
    "# Define Kafka source\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"your_kafka_topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka (which is in binary format) to string\n",
    "from pyspark.sql.functions import col\n",
    "streaming_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Assuming the data in value column is a JSON string, parse it into a DataFrame\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema of JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "parsed_df = streaming_df.withColumn(\"data\", from_json(col(\"value\"), schema)).select(\"data.*\")\n",
    "```\n",
    "\n",
    "Replace `\"your_kafka_topic\"` with the Kafka topic you are reading from and adjust the schema according to your data.\n",
    "\n",
    "#### 3. Write Streaming Data to Cassandra\n",
    "\n",
    "To write the streaming data to Cassandra, use the `.writeStream` method along with the Cassandra connector.\n",
    "\n",
    "```python\n",
    "# Write stream to Cassandra\n",
    "query = parsed_df.writeStream \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"your_table_name\", keyspace=\"your_keyspace_name\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Await termination of the streaming query\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "Replace `\"your_table_name\"` and `\"your_keyspace_name\"` with your Cassandra table and keyspace names. The checkpoint location is a directory on your file system or HDFS to store the checkpoint data.\n",
    "\n",
    "### Complete Example\n",
    "\n",
    "Here’s a complete example combining all the steps:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session with Cassandra connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraSparkStreamingIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka source\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"your_kafka_topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka (which is in binary format) to string\n",
    "streaming_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Define schema of JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON data\n",
    "parsed_df = streaming_df.withColumn(\"data\", from_json(col(\"value\"), schema)).select(\"data.*\")\n",
    "\n",
    "# Write stream to Cassandra\n",
    "query = parsed_df.writeStream \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"your_table_name\", keyspace=\"your_keyspace_name\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Await termination of the streaming query\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### Notes\n",
    "\n",
    "1. **Dependencies**: Make sure the `spark-cassandra-connector` version is compatible with your Spark and Scala versions.\n",
    "2. **Checkpointing**: Always provide a checkpoint location when working with streaming data to ensure fault tolerance and recovery.\n",
    "3. **Schema**: Adjust the schema to match your actual data structure.\n",
    "4. **Kafka Configuration**: Replace Kafka server and topic names with your actual Kafka configuration.\n",
    "\n",
    "This should help you set up streaming data integration between Kafka and Cassandra using PySpark. If you have further questions or need specific adjustments, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266883c6",
   "metadata": {},
   "source": [
    "**(C)**. Using `foreach()` or `foreachBatch()` in PySpark allows you to have more control over how you handle each row or each batch of data in your streaming job. These methods are particularly useful when you need to perform custom operations that aren't supported by the built-in output modes.\n",
    "\n",
    "### Using `foreach()`\n",
    "\n",
    "The `foreach()` method allows you to apply a function to each row of the streaming DataFrame or Dataset. This method is useful for custom row-level operations, but it can be less efficient than `foreachBatch()` because it operates on a row-by-row basis.\n",
    "\n",
    "#### Example with `foreach()`\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Cassandra connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraSparkStreamingIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka source\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"your_kafka_topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka (which is in binary format) to string\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema of JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON data\n",
    "streaming_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "parsed_df = streaming_df.withColumn(\"data\", from_json(col(\"value\"), schema)).select(\"data.*\")\n",
    "\n",
    "# Define a function to write each row to Cassandra\n",
    "def write_row_to_cassandra(row):\n",
    "    from cassandra.cluster import Cluster\n",
    "\n",
    "    # Connect to Cassandra\n",
    "    cluster = Cluster(['localhost'])\n",
    "    session = cluster.connect('your_keyspace_name')\n",
    "    \n",
    "    # Prepare the CQL statement\n",
    "    cql = \"INSERT INTO your_table_name (first_name, last_name, age) VALUES (%s, %s, %s)\"\n",
    "    \n",
    "    # Execute the statement with the row data\n",
    "    session.execute(cql, (row.first_name, row.last_name, row.age))\n",
    "\n",
    "# Use foreach to apply the function to each row\n",
    "query = parsed_df.writeStream \\\n",
    "    .foreach(write_row_to_cassandra) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "Replace `\"your_kafka_topic\"`, `\"your_keyspace_name\"`, and `\"your_table_name\"` with your actual Kafka topic, keyspace, and table names.\n",
    "\n",
    "### Using `foreachBatch()`\n",
    "\n",
    "The `foreachBatch()` method allows you to apply a function to each batch of the streaming DataFrame or Dataset. This method is more efficient than `foreach()` because it processes the data in batches. It is well-suited for writing to databases like Cassandra.\n",
    "\n",
    "#### Example with `foreachBatch()`\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Cassandra connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraSparkStreamingIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka source\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"your_kafka_topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka (which is in binary format) to string\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema of JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON data\n",
    "streaming_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "parsed_df = streaming_df.withColumn(\"data\", from_json(col(\"value\"), schema)).select(\"data.*\")\n",
    "\n",
    "# Define a function to write each batch to Cassandra\n",
    "def write_batch_to_cassandra(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=\"your_table_name\", keyspace=\"your_keyspace_name\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "# Use foreachBatch to apply the function to each batch\n",
    "query = parsed_df.writeStream \\\n",
    "    .foreachBatch(write_batch_to_cassandra) \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "Replace `\"your_kafka_topic\"`, `\"your_keyspace_name\"`, and `\"your_table_name\"` with your actual Kafka topic, keyspace, and table names. Ensure you specify a checkpoint location to maintain the streaming state.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **`foreach()`**: Use for custom row-level operations. Less efficient for large-scale operations because it processes data row-by-row.\n",
    "- **`foreachBatch()`**: Use for custom batch-level operations. More efficient for large-scale operations because it processes data in batches.\n",
    "\n",
    "Both methods give you the flexibility to handle streaming data in a way that suits your specific needs. If you need further assistance or have specific use cases, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ab4b8",
   "metadata": {},
   "source": [
    "In Spark Structured Streaming, the `foreachBatch` method automatically passes the current batch DataFrame (`batch_df`) and the batch ID (`batch_id`) to the function you define. This is why you don't need to explicitly pass these arguments when calling the function in your streaming query. \n",
    "\n",
    "Here’s a step-by-step explanation of how it works:\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - `write_batch_to_cassandra(batch_df, batch_id)`: This function is defined to take two parameters:\n",
    "     - `batch_df`: The DataFrame representing the current batch of streaming data.\n",
    "     - `batch_id`: A unique identifier for the batch.\n",
    "\n",
    "2. **Using `foreachBatch`**:\n",
    "   - When you use `foreachBatch`, Spark Structured Streaming will call the provided function for each batch of data. It will automatically pass the `batch_df` and `batch_id` to the function.\n",
    "\n",
    "### Complete Example\n",
    "\n",
    "Here is a complete example illustrating how to use `foreachBatch` to write streaming data to Cassandra:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session with Cassandra connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CassandraSparkStreamingIntegration\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.2.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka source\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"your_kafka_topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the value column from Kafka (which is in binary format) to string\n",
    "# Define schema of JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Parse JSON data\n",
    "streaming_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "parsed_df = streaming_df.withColumn(\"data\", from_json(col(\"value\"), schema)).select(\"data.*\")\n",
    "\n",
    "# Define a function to write each batch to Cassandra\n",
    "def write_batch_to_cassandra(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=\"your_table_name\", keyspace=\"your_keyspace_name\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "# Use foreachBatch to apply the function to each batch\n",
    "query = parsed_df.writeStream \\\n",
    "    .foreachBatch(write_batch_to_cassandra) \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "- **`foreachBatch` Method**: This method will call the `write_batch_to_cassandra` function for each batch of data received from the streaming source.\n",
    "- **Automatic Argument Passing**: Spark automatically passes `batch_df` (the DataFrame for the current batch) and `batch_id` (a unique ID for the batch) to the function.\n",
    "- **Checkpointing**: The checkpoint location is specified to maintain the state of the streaming query. It’s necessary for fault tolerance.\n",
    "\n",
    "By using `foreachBatch`, you gain the flexibility to perform batch-level operations efficiently, making it ideal for writing data to external systems like Cassandra in a structured and controlled manner. If you have further questions or specific requirements, feel free to ask!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
