{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980d9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debbaa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/xwyang/anaconda3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/xwyang/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/xwyang/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3c75e663-3f9b-4213-8bef-185aa03d2f5a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 221ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3c75e663-3f9b-4213-8bef-185aa03d2f5a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "24/07/22 22:01:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session with Delta Lake configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.2.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62df3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLocalProperty('spark.scheduler.pool','pool_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe86e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/Users/xwyang/Desktop/data/flights.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f682966d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67283ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'delay', 'distance', 'origin', 'destination']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc0a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfc38132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format('parquet').mode('overwrite').saveAsTable('pooltbl_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43389469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1010630|  -10|     928|   RSW|        EWR|\n",
      "|1021029|   87|     974|   RSW|        ORD|\n",
      "|1021346|    0|     928|   RSW|        EWR|\n",
      "|1021044|   18|     928|   RSW|        EWR|\n",
      "|1021730|   29|     748|   RSW|        IAH|\n",
      "|1020535|  605|     974|   RSW|        ORD|\n",
      "|1021820|   71|     974|   RSW|        ORD|\n",
      "|1021743|    0|     928|   RSW|        EWR|\n",
      "|1022017|    0|     928|   RSW|        EWR|\n",
      "|1020600|   -2|     748|   RSW|        IAH|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('pooltbl_1').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb314311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"  create table if not exists pool_scheduler_tbl \n",
    "using csv options(path'/Users/xwyang/Desktop/data/flights.csv',header=True,inferSchema=True)    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede4f8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table('pool_scheduler_tbl').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccfd4c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+-------+\n",
      "|   date|delay|distance|origin|destination| status|\n",
      "+-------+-----+--------+------+-----------+-------+\n",
      "|1011245|    6|     602|   ABE|        ATL|  delay|\n",
      "|1020600|   -8|     369|   ABE|        DTW|  Early|\n",
      "|1021245|   -2|     602|   ABE|        ATL|  Early|\n",
      "|1020605|   -4|     602|   ABE|        ATL|  Early|\n",
      "|1031245|   -4|     602|   ABE|        ATL|  Early|\n",
      "|1030605|    0|     602|   ABE|        ATL|  delay|\n",
      "|1041243|   10|     602|   ABE|        ATL|  delay|\n",
      "|1040605|   28|     602|   ABE|        ATL|on-time|\n",
      "|1051245|   88|     602|   ABE|        ATL|on-time|\n",
      "|1050605|    9|     602|   ABE|        ATL|  delay|\n",
      "+-------+-----+--------+------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select date,delay,distance,origin,destination,\n",
    "case when delay < 0 then'Early' \n",
    "     when delay between 20 and 100 then'on-time'\n",
    "     when delay >100 and delay <200 then 'minor-delay'\n",
    "     Else 'delay' end as status\n",
    "     from pool_scheduler_tbl\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ac2dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------+\n",
      "|namespace|         tableName|isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|  default|pool_scheduler_tbl|      false|\n",
      "|  default|         pooltbl_1|      false|\n",
      "+---------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c481807",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaPath='/Users/xwyang/Desktop/delta_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0c6474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = spark.table('pooltbl_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b17933c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1010630|  -10|     928|   RSW|        EWR|\n",
      "|1021029|   87|     974|   RSW|        ORD|\n",
      "|1021346|    0|     928|   RSW|        EWR|\n",
      "|1021044|   18|     928|   RSW|        EWR|\n",
      "|1021730|   29|     748|   RSW|        IAH|\n",
      "|1020535|  605|     974|   RSW|        ORD|\n",
      "|1021820|   71|     974|   RSW|        ORD|\n",
      "|1021743|    0|     928|   RSW|        EWR|\n",
      "|1022017|    0|     928|   RSW|        EWR|\n",
      "|1020600|   -2|     748|   RSW|        IAH|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49e71936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/22 22:04:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dff.write.format('delta').mode('overwrite').save(deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b26b4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "582943ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df = spark.read.format('delta').load(deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92546cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:===============================================>        (42 + 4) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|3300630|   -4|     473|   LGA|        CLT|\n",
      "|3301200|   -2|     160|   LGA|        BOS|\n",
      "|3301400|   -5|     160|   LGA|        BOS|\n",
      "|3301600|   46|     160|   LGA|        BOS|\n",
      "|3301800|    0|     160|   LGA|        BOS|\n",
      "|3302000|   69|     160|   LGA|        BOS|\n",
      "|3300900|  -10|     186|   LGA|        DCA|\n",
      "|3301100|   15|     186|   LGA|        DCA|\n",
      "|3301200|    2|     186|   LGA|        DCA|\n",
      "|3301300|   -5|     186|   LGA|        DCA|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48b427a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b64927f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [date#1373,delay#1374,distance#1375,origin#1376,destination#1377] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/Users/xwyang/Desktop/delta_dir], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<date:int,delay:int,distance:int,origin:string,destination:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc3706e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('delta').load(deltaPath).createOrReplaceTempView('delta_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3127d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-----------+\n",
      "|namespace|         tableName|isTemporary|\n",
      "+---------+------------------+-----------+\n",
      "|  default|pool_scheduler_tbl|      false|\n",
      "|  default|         pooltbl_1|      false|\n",
      "|         |         delta_tbl|      false|\n",
      "+---------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da5e2048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|3300630|   -4|     473|   LGA|        CLT|\n",
      "|3301200|   -2|     160|   LGA|        BOS|\n",
      "|3301400|   -5|     160|   LGA|        BOS|\n",
      "|3301600|   46|     160|   LGA|        BOS|\n",
      "|3301800|    0|     160|   LGA|        BOS|\n",
      "+-------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select * from delta_tbl limit 5  \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83032c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|total_counts|\n",
      "+------------+\n",
      "|     1391578|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"  select count(*) as total_counts from delta_tbl\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "543b165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|origin|total_counts|\n",
      "+------+------------+\n",
      "|   ATL|       91484|\n",
      "|   DFW|       68482|\n",
      "|   ORD|       64228|\n",
      "|   LAX|       54086|\n",
      "|   DEN|       53148|\n",
      "|   IAH|       43361|\n",
      "|   PHX|       40155|\n",
      "|   SFO|       39483|\n",
      "|   LAS|       33107|\n",
      "|   CLT|       28402|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"  select origin, count(*) as total_counts \n",
    "from delta_tbl group by origin order by total_counts DESC limit 10\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394aa9d",
   "metadata": {},
   "source": [
    "**Loading Data Streams into a Delta Lake Table**\n",
    "As with static DataFrames, you can easily modify your eisting Structured Streaming jobs to write to and read from a Delta Lake table by setting the format to 'delta'. Say you have a stream of data as a DataFrame named NewStreamDF,which has the same schema as the table. You can append to the table as follows:\n",
    "\n",
    "```\n",
    "\n",
    " delta_query = (NewStreamDF.writeStream\n",
    "                           .format('delta')\n",
    "                           .outputMode('append')\n",
    "                           .option('checkpointLocation',check_dir)\n",
    "                           .trigger(processingTime='10 seconds')\n",
    "                           .start(deltaPath))\n",
    " ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
