{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a9b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer,OneHotEncoder,RFormula\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1935cdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/11 21:46:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName('my-spark-ML')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921d4a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/Users/xwyang/Desktop/data/sf-airbnb.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8839f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/11 21:46:40 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format('parquet').mode('overwrite').save('/Users/xwyang/Desktop/parquet_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71e1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = spark.read.parquet('/Users/xwyang/Desktop/parquet_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8de880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = spark.read.parquet('/Users/xwyang/Desktop/myspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91916627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b59be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" create or replace temp view sf_tbl using parquet options(path '/Users/xwyang/Desktop/myspace') \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37920919",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = spark.table('sf_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f738cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" create or replace temp view sf_tbl_1 as select * from sf_tbl \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e7bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.table('sf_tbl_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1226dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "savePipe = PipelineModel.load('/Users/xwyang/Desktop/sf-airbnb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee8364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF,testDF = dff.randomSplit([0.8,0.2],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d13d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predDF = savePipe.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e214518",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='price',\n",
    "                                metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10d2b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rsme = evaluation.evaluate(predDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f2c0318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218.01859667614502"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bc34143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240.70934302168342"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_value = trainDF.select(avg('price')).collect()[0][0]\n",
    "baseline_df = testDF.withColumn('new_pred',lit(mean_value))\n",
    "evaluation_1 = RegressionEvaluator(predictionCol='new_pred',\n",
    "                                labelCol='price',\n",
    "                                metricName='rmse')\n",
    "rmse=evaluation_1.evaluate(baseline_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93416309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17969345201487363"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsem_r2 = evaluation.setMetricName(\"r2\").evaluate(predDF)\n",
    "rsem_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f63436",
   "metadata": {},
   "source": [
    "Below is a list of the commonly used machine learning models in PySpark, along with their key import statements and important parameters that you would typically need to set up each model.\n",
    "\n",
    "## Classification Models\n",
    "\n",
    "1. **Logistic Regression**\n",
    "   ```python\n",
    "   from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "   lr = LogisticRegression(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       maxIter=10,\n",
    "       regParam=0.3,\n",
    "       elasticNetParam=0.8\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Decision Tree Classifier**\n",
    "   ```python\n",
    "   from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "   dt = DecisionTreeClassifier(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       maxDepth=5,\n",
    "       impurity='gini'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Random Forest Classifier**\n",
    "   ```python\n",
    "   from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "   rf = RandomForestClassifier(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       numTrees=20,\n",
    "       maxDepth=5\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Gradient-Boosted Tree Classifier**\n",
    "   ```python\n",
    "   from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "   gbt = GBTClassifier(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       maxIter=10,\n",
    "       maxDepth=5\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. **Naive Bayes**\n",
    "   ```python\n",
    "   from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "   nb = NaiveBayes(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       smoothing=1.0,\n",
    "       modelType='multinomial'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "6. **Multilayer Perceptron Classifier (Neural Network)**\n",
    "   ```python\n",
    "   from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "   mlp = MultilayerPerceptronClassifier(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       layers=[4, 5, 4, 3],  # Example architecture with input layer, 2 hidden layers, and output layer\n",
    "       maxIter=100\n",
    "   )\n",
    "   ```\n",
    "\n",
    "7. **Linear Support Vector Machine (SVM)**\n",
    "   ```python\n",
    "   from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "   lsvc = LinearSVC(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       maxIter=10,\n",
    "       regParam=0.1\n",
    "   )\n",
    "   ```\n",
    "\n",
    "## Regression Models\n",
    "\n",
    "1. **Linear Regression**\n",
    "   ```python\n",
    "   from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "   lr = LinearRegression(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       maxIter=10,\n",
    "       regParam=0.3,\n",
    "       elasticNetParam=0.8\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Decision Tree Regressor**\n",
    "   ```python\n",
    "   from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "   dtr = DecisionTreeRegressor(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       maxDepth=5,\n",
    "       impurity='variance'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Random Forest Regressor**\n",
    "   ```python\n",
    "   from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "   rfr = RandomForestRegressor(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       numTrees=20,\n",
    "       maxDepth=5\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Gradient-Boosted Tree Regressor**\n",
    "   ```python\n",
    "   from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "   gbtr = GBTRegressor(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       maxIter=10,\n",
    "       maxDepth=5\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. **Isotonic Regression**\n",
    "   ```python\n",
    "   from pyspark.ml.regression import IsotonicRegression\n",
    "\n",
    "   ir = IsotonicRegression(\n",
    "       featuresCol='features',\n",
    "       labelCol='label'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "6. **Generalized Linear Regression**\n",
    "   ```python\n",
    "   from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "   glr = GeneralizedLinearRegression(\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       family='gaussian',\n",
    "       link='identity',\n",
    "       maxIter=10,\n",
    "       regParam=0.3\n",
    "   )\n",
    "   ```\n",
    "\n",
    "## Clustering Models\n",
    "\n",
    "1. **K-Means Clustering**\n",
    "   ```python\n",
    "   from pyspark.ml.clustering import KMeans\n",
    "\n",
    "   kmeans = KMeans(\n",
    "       featuresCol='features',\n",
    "       k=3,\n",
    "       maxIter=20,\n",
    "       seed=1\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Gaussian Mixture Model (GMM)**\n",
    "   ```python\n",
    "   from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "   gmm = GaussianMixture(\n",
    "       featuresCol='features',\n",
    "       k=2,\n",
    "       maxIter=100\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Bisecting K-Means**\n",
    "   ```python\n",
    "   from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "   bkm = BisectingKMeans(\n",
    "       featuresCol='features',\n",
    "       k=2,\n",
    "       maxIter=20\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Latent Dirichlet Allocation (LDA)**\n",
    "   ```python\n",
    "   from pyspark.ml.clustering import LDA\n",
    "\n",
    "   lda = LDA(\n",
    "       featuresCol='features',\n",
    "       k=3,\n",
    "       maxIter=10\n",
    "   )\n",
    "   ```\n",
    "\n",
    "## Recommendation Models\n",
    "\n",
    "1. **Alternating Least Squares (ALS) for Collaborative Filtering**\n",
    "   ```python\n",
    "   from pyspark.ml.recommendation import ALS\n",
    "\n",
    "   als = ALS(\n",
    "       userCol='userId',\n",
    "       itemCol='movieId',\n",
    "       ratingCol='rating',\n",
    "       maxIter=10,\n",
    "       regParam=0.01,\n",
    "       coldStartStrategy='drop'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "## Feature Extraction and Transformation Models\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import PCA\n",
    "\n",
    "   pca = PCA(\n",
    "       k=3,\n",
    "       inputCol='features',\n",
    "       outputCol='pcaFeatures'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Word2Vec**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "   word2vec = Word2Vec(\n",
    "       vectorSize=100,\n",
    "       inputCol='text',\n",
    "       outputCol='result'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. **Chi-Squared Selector**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "   selector = ChiSqSelector(\n",
    "       numTopFeatures=50,\n",
    "       featuresCol='features',\n",
    "       labelCol='label',\n",
    "       outputCol='selectedFeatures'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. **Standard Scaler**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "   scaler = StandardScaler(\n",
    "       inputCol='features',\n",
    "       outputCol='scaledFeatures',\n",
    "       withMean=True,\n",
    "       withStd=True\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. **MinMax Scaler**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "   scaler = MinMaxScaler(\n",
    "       inputCol='features',\n",
    "       outputCol='scaledFeatures'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "6. **MaxAbs Scaler**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "   scaler = MaxAbsScaler(\n",
    "       inputCol='features',\n",
    "       outputCol='scaledFeatures'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "7. **Bucketizer**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "   bucketizer = Bucketizer(\n",
    "       splits=[-float('inf'), 0.0, float('inf')],\n",
    "       inputCol='features',\n",
    "       outputCol='bucketedFeatures'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "8. **Quantile Discretizer**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "   discretizer = QuantileDiscretizer(\n",
    "       numBuckets=3,\n",
    "       inputCol='features',\n",
    "       outputCol='bucketedFeatures'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "9. **Polynomial Expansion**\n",
    "   ```python\n",
    "   from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "   polyExpansion = PolynomialExpansion(\n",
    "       degree=2,\n",
    "       inputCol='features',\n",
    "       outputCol='polyFeatures'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### Summary\n",
    "For each model, you should import the corresponding class and initialize it with the parameters that best suit your data and problem. You can adjust the parameters according to your needs to control aspects such as model complexity, regularization, maximum iterations, etc. These parameters are often the key to tuning and optimizing your model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
