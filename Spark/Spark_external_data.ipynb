{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a590a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; import numpy as np\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3c9921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/28 20:31:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "        appName('spark_external_data').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1275185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/Users/xwyang/Desktop/data/flight_delays.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c580548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'delay', 'distance', 'origin', 'destination']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e892088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae85c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create database if not exists my_spark_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc3e6328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('use my_spark_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ee4168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd66f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.saveAsTable('flys_tbl_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4e937d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+\n",
      "|  namespace| tableName|isTemporary|\n",
      "+-----------+----------+-----------+\n",
      "|my_spark_db|flys_tbl_1|      false|\n",
      "+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91c407cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fly_delays_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f256af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------+\n",
      "|  namespace|     tableName|isTemporary|\n",
      "+-----------+--------------+-----------+\n",
      "|my_spark_db|    flys_tbl_1|      false|\n",
      "|           |fly_delays_tbl|       true|\n",
      "+-----------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d243d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" create table if not exists flight_delay_tbl(date string,\n",
    " delay int, distance int, origin string, destination string)\n",
    "            using csv options(path '/Users/xwyang/Desktop/data/flight_delays.csv',\n",
    "            header=True, inferSchema=True)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abc2d134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from flight_delay_tbl limit 3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "572c9878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1391578"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1afa2796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+\n",
      "|  namespace|       tableName|isTemporary|\n",
      "+-----------+----------------+-----------+\n",
      "|my_spark_db|flight_delay_tbl|      false|\n",
      "|my_spark_db|      flys_tbl_1|      false|\n",
      "|           |  fly_delays_tbl|       true|\n",
      "+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51e77269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('alter table flys_tbl_1 add column status_update string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "903efe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+-------------+\n",
      "|   date|delay|distance|origin|destination|status_update|\n",
      "+-------+-----+--------+------+-----------+-------------+\n",
      "|1010630|  -10|     928|   RSW|        EWR|         NULL|\n",
      "|1021029|   87|     974|   RSW|        ORD|         NULL|\n",
      "+-------+-----+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from flys_tbl_1 limit 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41846811",
   "metadata": {},
   "source": [
    "## To connect to a MySQL database via spark SQL\n",
    "\n",
    " ### spark = SparkSession\\\n",
    "          .builder\\\n",
    "          .appName('mysql_spark')\\\n",
    "          .config(\"spark.jars\",\"/path/to/mysql-connector-java-8.0.26.jar\")\\\n",
    "          .getOrCreate()\n",
    " . Load  data from and save it to MySQL database using the Spark SQL  data source API and JDBC \n",
    "\n",
    "  ### spark.read.format('jdbc')\\\n",
    "       .option('url','jdbc:mysql://[hostname]:3306/[mydatabase]')\\\n",
    "       .option('driver','com.mysql.cj.jdbc.Driver')\\\n",
    "       .option('dbtable','[tablename]')\\\n",
    "       .option('user','[username]')\\\n",
    "       .option('password','[password]')\\\n",
    "       .load()\n",
    " \n",
    " connection_properties={\n",
    " \n",
    "### spark.read.jdbc\n",
    "        (url=mysql_url, \n",
    "         table='mytable',\n",
    "         properties=connection_properties, \n",
    "         mode='append',# 'overwrite,error,ignor'\n",
    "         numPartition=10,\n",
    "         lowerBound=0,\n",
    "         upperBound=10000,\n",
    "         parttionColumn='id')\n",
    "         \n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## Initialize SparkSession with the MySQL JDBC driver specified in the spark.jars configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Write_DataFrame_to_JDBC\") \\\n",
    "    .config(\"spark.jars\", \"/path/to/mysql-connector-java-8.0.27.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## MySQL connection properties\n",
    "mysql_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "\n",
    "mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "## Define partitioning properties\n",
    "partitionColumn = \"id\"  # The column to partition by\n",
    "\n",
    "numPartitions = 10\n",
    "\n",
    "lowerBound = 1\n",
    "\n",
    "upperBound = 1000000\n",
    "\n",
    "## Read data from MySQL table into a Spark DataFrame with partitioning\n",
    "df = spark.read.jdbc(\n",
    "\n",
    "    url=mysql_url,\n",
    "    table=\"employees\",\n",
    "    properties=mysql_properties,\n",
    "    column=partitionColumn,\n",
    "    lowerBound=lowerBound,\n",
    "    upperBound=upperBound,\n",
    "    numPartitions=numPartitions\n",
    ")\n",
    "\n",
    "## Repartition the DataFrame\n",
    "df = df.repartition(numPartitions)\n",
    "\n",
    "## Define JDBC write properties\n",
    "jdbc_write_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "## Define the URL for the JDBC connection\n",
    "jdbc_url = \"jdbc:mysql://localhost:3306/mydatabase\"\n",
    "\n",
    "## Write the DataFrame to the MySQL database table with partitioning\n",
    "df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"employees\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=jdbc_write_properties\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66b1ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|department|employee_name|\n",
      "+----------+-------------+\n",
      "|        HR|        Alice|\n",
      "|        HR|          Bob|\n",
      "|        IT|      Charlie|\n",
      "|        IT|        David|\n",
      "|        IT|          Eve|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    Row(department=\"HR\", employee_name=\"Alice\"),\n",
    "    Row(department=\"HR\", employee_name=\"Bob\"),\n",
    "    Row(department=\"IT\", employee_name=\"Charlie\"),\n",
    "    Row(department=\"IT\", employee_name=\"David\"),\n",
    "    Row(department=\"IT\", employee_name=\"Eve\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcf6f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.sql(\"\"\" select department,collect_list(employee_name) as employee_names \n",
    "          from employees group by department \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bce7db2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "|department|employee_names       |\n",
      "+----------+---------------------+\n",
      "|HR        |[Alice, Bob]         |\n",
      "|IT        |[Charlie, David, Eve]|\n",
      "+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71290593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|department|employee_name|\n",
      "+----------+-------------+\n",
      "|        HR|        Alice|\n",
      "|        HR|          Bob|\n",
      "|        IT|      Charlie|\n",
      "|        IT|        David|\n",
      "|        IT|          Eve|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select('department',explode(col('employee_names')).alias('employee_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ba9bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('long')\n",
    "def plus_one(v):\n",
    "    return v+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5489f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|plus_one|\n",
      "+---+--------+\n",
      "|  0|       1|\n",
      "|  1|       2|\n",
      "|  2|       3|\n",
      "|  3|       4|\n",
      "|  4|       5|\n",
      "+---+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xwyang/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/xwyang/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/xwyang/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/Users/xwyang/anaconda3/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dw = spark.range(5)\n",
    "dw.withColumn('plus_one',plus_one('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fa8620a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['department', 'employee_name']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5939e065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['department', 'employee_names']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24219901",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = spark.sql(\"select * from flys_tbl_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b7fc98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+-------------+\n",
      "|   date|delay|distance|origin|destination|status_update|\n",
      "+-------+-----+--------+------+-----------+-------------+\n",
      "|1010630|  -10|     928|   RSW|        EWR|         NULL|\n",
      "|1021029|   87|     974|   RSW|        ORD|         NULL|\n",
      "+-------+-----+--------+------+-----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53bff726",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = da.groupBy('origin').agg(collect_list(col('delay')).alias('delay_orgi')).orderBy('delay_orgi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3cc9a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|origin|          delay_orgi|\n",
      "+------+--------------------+\n",
      "|   YAK|[-25, -42, -14, 0...|\n",
      "|   OME|[-21, -18, -13, -...|\n",
      "+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "991fae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/28 22:41:21 WARN DAGScheduler: Broadcasting large task binary with size 4.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|origin|col|\n",
      "+------+---+\n",
      "|   YAK|-25|\n",
      "|   YAK|-42|\n",
      "+------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.select('origin',explode('delay_orgi')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8515e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|origin|          delay_orgi|\n",
      "+------+--------------------+\n",
      "|   YAK|[-25, -42, -14, 0...|\n",
      "|   OME|[-21, -18, -13, -...|\n",
      "+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dd.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "528ad824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------+\n",
      "|origin|array_min(delay_orgi)|\n",
      "+------+---------------------+\n",
      "|   YAK|                  -53|\n",
      "|   OME|                  -28|\n",
      "+------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.select('origin',array_min('delay_orgi')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f97402f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- origin: string (nullable = true)\n",
      " |-- delay_orgi: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b0be9400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|array_distinct(array(1, 2, 3, NULL, 3))|\n",
      "+---------------------------------------+\n",
      "|                        [1, 2, 3, NULL]|\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql( \" select array_distinct(array(1,2,3,null,3))\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "86264b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|date                                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[2022-01-01, 2022-01-02, 2022-01-03, 2022-01-04, 2022-01-05]|\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sequence(to_date('2022-01-01'),to_date('2022-01-5'),interval 1 day) as date \").show(2,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
