{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436c6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cdacdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/06 07:02:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName('spark-joins')\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\",'true')\\\n",
    "        .config(\"spark.executor.memory\",'2g')\\\n",
    "        .config(\"spark.executor.cores\",'2')\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91349b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {0: \"AZ\", 1: \"CO\", 2: \"CA\", 3: \"TX\", 4: \"NY\", 5: \"MI\"}\n",
    "items = {0: \"SKU-0\", 1: \"SKU-1\", 2: \"SKU-2\", 3: \"SKU-3\", 4: \"SKU-4\", 5: \"SKU-5\"}\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8207b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = [(id, f\"user_{id}\", f\"user_{id}@databricks.com\",\n",
    "                   states[np.random.randint(0, 5)]) for id in range(100001)]\n",
    "orders_data = [(r, r, np.random.randint(0, 100000), 10 * r * 0.2, \n",
    "                    states[np.random.randint(0, 5)], items[np.random.randint(0, 5)]) for r in range(100001)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b7a655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 75885, 0.0, 'AZ', 'SKU-2'),\n",
       " (1, 1, 69179, 2.0, 'TX', 'SKU-2'),\n",
       " (2, 2, 25773, 4.0, 'AZ', 'SKU-3'),\n",
       " (3, 3, 12681, 6.0, 'CA', 'SKU-3')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c37e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'user_0', 'user_0@databricks.com', 'TX'),\n",
       " (1, 'user_1', 'user_1@databricks.com', 'NY'),\n",
       " (2, 'user_2', 'user_2@databricks.com', 'CA'),\n",
       " (3, 'user_3', 'user_3@databricks.com', 'NY')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0612522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_1=\" uid int,login string,email string,user_state string   \"\n",
    "schema_2 = \"transaction_id int,quantity int,user_id int,amount float,state string,items string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33302b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDf = spark.createDataFrame(users_data,schema_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84412d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 07:02:42 WARN TaskSetManager: Stage 0 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------------+----------+\n",
      "|uid|login |email                |user_state|\n",
      "+---+------+---------------------+----------+\n",
      "|0  |user_0|user_0@databricks.com|TX        |\n",
      "|1  |user_1|user_1@databricks.com|NY        |\n",
      "|2  |user_2|user_2@databricks.com|CA        |\n",
      "+---+------+---------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDf.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfb8264b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uid: int, login: string, email: string, user_state: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersDf.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb3333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf = spark.createDataFrame(orders_data,schema_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d240ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+-------+------+-----+-----+\n",
      "|transaction_id|quantity|user_id|amount|state|items|\n",
      "+--------------+--------+-------+------+-----+-----+\n",
      "|0             |0       |75885  |0.0   |AZ   |SKU-2|\n",
      "|1             |1       |69179  |2.0   |TX   |SKU-2|\n",
      "+--------------+--------+-------+------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDf.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e40ff2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.join.preferSortMergeJoin','true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a9d102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transaction_id: int, quantity: int, user_id: int, amount: float, state: string, items: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersDf.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e31ff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98ae7c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 07:26:01 WARN TaskSetManager: Stage 6 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/06 07:26:02 WARN TaskSetManager: Stage 7 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ba394d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 07:27:29 WARN TaskSetManager: Stage 10 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(uid)|\n",
      "+----------+\n",
      "|    100001|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDf.select(count(col('uid'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a39497d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+-------+--------+-----+-----+\n",
      "|transaction_id|quantity|user_id|  amount|state|items|\n",
      "+--------------+--------+-------+--------+-----+-----+\n",
      "|         58100|   58100|      0|116200.0|   CA|SKU-1|\n",
      "|         19229|   19229|      1| 38458.0|   NY|SKU-1|\n",
      "|         81594|   81594|      1|163188.0|   TX|SKU-1|\n",
      "+--------------+--------+-------+--------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDf.orderBy('user_id').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db07f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_orderDf = usersDf.join(ordersDf,usersDf.uid==ordersDf.user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "369ebb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+----------+\n",
      "|uid| login|               email|user_state|\n",
      "+---+------+--------------------+----------+\n",
      "|  0|user_0|user_0@databricks...|        TX|\n",
      "|  1|user_1|user_1@databricks...|        NY|\n",
      "+---+------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 07:36:03 WARN TaskSetManager: Stage 14 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "usersDf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8701026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 07:36:23 WARN TaskSetManager: Stage 15 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_orderDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e10e473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 07:36:59 WARN TaskSetManager: Stage 24 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 25:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------------+----------+--------------+--------+-------+--------+-----+-----+\n",
      "|uid|login |email                |user_state|transaction_id|quantity|user_id|amount  |state|items|\n",
      "+---+------+---------------------+----------+--------------+--------+-------+--------+-----+-----+\n",
      "|1  |user_1|user_1@databricks.com|NY        |19229         |19229   |1      |38458.0 |NY   |SKU-1|\n",
      "|1  |user_1|user_1@databricks.com|NY        |81594         |81594   |1      |163188.0|TX   |SKU-1|\n",
      "+---+------+---------------------+----------+--------------+--------+-------+--------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_orderDf.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "204fadba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [uid#0], [user_id#47], Inner\n",
      "   :- Sort [uid#0 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(uid#0, 200), ENSURE_REQUIREMENTS, [plan_id=671]\n",
      "   :     +- Filter isnotnull(uid#0)\n",
      "   :        +- InMemoryTableScan [uid#0, login#1, email#2, user_state#3], [isnotnull(uid#0)]\n",
      "   :              +- InMemoryRelation [uid#0, login#1, email#2, user_state#3], StorageLevel(disk, 1 replicas)\n",
      "   :                    +- *(1) Scan ExistingRDD[uid#0,login#1,email#2,user_state#3]\n",
      "   +- Sort [user_id#47 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(user_id#47, 200), ENSURE_REQUIREMENTS, [plan_id=672]\n",
      "         +- Filter isnotnull(user_id#47)\n",
      "            +- InMemoryTableScan [transaction_id#45, quantity#46, user_id#47, amount#48, state#49, items#50], [isnotnull(user_id#47)]\n",
      "                  +- InMemoryRelation [transaction_id#45, quantity#46, user_id#47, amount#48, state#49, items#50], StorageLevel(disk, 1 replicas)\n",
      "                        +- *(1) Scan ExistingRDD[transaction_id#45,quantity#46,user_id#47,amount#48,state#49,items#50]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_orderDf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b961388b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ordersDf.orderBy('user_id').write\\\n",
    "        .format('parquet')\\\n",
    "        .bucketBy(8,'user_id')\\\n",
    "        .saveAsTable('order_tbl',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e0152d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/06 08:15:16 WARN TaskSetManager: Stage 33 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/06 08:15:16 WARN TaskSetManager: Stage 34 contains a task of very large size (1162 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usersDf.orderBy('uid').write\\\n",
    "        .format('parquet')\\\n",
    "        .bucketBy(8,'uid')\\\n",
    "        .saveAsTable('user_tbl',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bd79ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CACHE TABLE User_Tbl\")\n",
    "spark.sql(\"CACHE TABLE Order_Tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2688d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.table('order_tbl')\n",
    "users_df = spark.table('user_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3ee8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinur = orders_df.join(users_df,orders_df.user_id==users_df.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "694a4783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+-------+--------+-----+-----+---+-------+----------------------+----------+\n",
      "|transaction_id|quantity|user_id|amount  |state|items|uid|login  |email                 |user_state|\n",
      "+--------------+--------+-------+--------+-----+-----+---+-------+----------------------+----------+\n",
      "|57833         |57833   |13     |115666.0|NY   |SKU-2|13 |user_13|user_13@databricks.com|CO        |\n",
      "|33546         |33546   |38     |67092.0 |TX   |SKU-3|38 |user_38|user_38@databricks.com|AZ        |\n",
      "|81612         |81612   |70     |163224.0|AZ   |SKU-4|70 |user_70|user_70@databricks.com|AZ        |\n",
      "+--------------+--------+-------+--------+-----+-----+---+-------+----------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinur.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42989109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [user_id#1860], [uid#1721], Inner\n",
      "   :- Sort [user_id#1860 ASC NULLS FIRST], false, 0\n",
      "   :  +- Filter isnotnull(user_id#1860)\n",
      "   :     +- Scan In-memory table Order_Tbl [transaction_id#1858, quantity#1859, user_id#1860, amount#1861, state#1862, items#1863], [isnotnull(user_id#1860)]\n",
      "   :           +- InMemoryRelation [transaction_id#1858, quantity#1859, user_id#1860, amount#1861, state#1862, items#1863], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- *(1) ColumnarToRow\n",
      "   :                    +- FileScan parquet spark_catalog.default.order_tbl[transaction_id#1858,quantity#1859,user_id#1860,amount#1861,state#1862,items#1863] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/xwyang/spark-warehouse/order_tbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,quantity:int,user_id:int,amount:float,state:string,items:string>, SelectedBucketsCount: 8 out of 8\n",
      "   +- Sort [uid#1721 ASC NULLS FIRST], false, 0\n",
      "      +- Filter isnotnull(uid#1721)\n",
      "         +- Scan In-memory table User_Tbl [uid#1721, login#1722, email#1723, user_state#1724], [isnotnull(uid#1721)]\n",
      "               +- InMemoryRelation [uid#1721, login#1722, email#1723, user_state#1724], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) ColumnarToRow\n",
      "                        +- FileScan parquet spark_catalog.default.user_tbl[uid#1721,login#1722,email#1723,user_state#1724] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/xwyang/spark-warehouse/user_tbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<uid:int,login:string,email:string,user_state:string>, SelectedBucketsCount: 8 out of 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinur.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "145be15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|order_tbl|      false|\n",
      "|  default| user_tbl|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef494b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col_name: string, data_type: string, comment: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('describe table user_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a37df4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" create or replace temp view \n",
    "fly_tbl using csv options(path '/Users/xwyang/Desktop/data/flight_delays.csv')  \"\"\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f194506",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = spark.read.csv('/Users/xwyang/Desktop/data/flight_delays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02a78d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|order_tbl|      false|\n",
      "|  default| user_tbl|      false|\n",
      "|         |  fly_tbl|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2557e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
