{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b87b20a",
   "metadata": {},
   "source": [
    "To read from and write to Kafka using PySpark in a Jupyter Notebook, follow these steps:\n",
    "\n",
    "### Step 1: Install Required Libraries\n",
    "\n",
    "Ensure you have the required libraries installed. You need `pyspark` and `kafka-python`. You can install them using pip.\n",
    "\n",
    "```bash\n",
    "!pip install pyspark kafka-python\n",
    "```\n",
    "\n",
    "### Step 2: Set Up Your Kafka Environment\n",
    "\n",
    "Make sure you have a Kafka broker running and accessible. For local development, you can use tools like Docker to set up Kafka.\n",
    "\n",
    "### Step 3: Import Required Libraries\n",
    "\n",
    "In your Jupyter Notebook, import the necessary libraries from PySpark.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "```\n",
    "\n",
    "### Step 4: Create a SparkSession\n",
    "\n",
    "Create a SparkSession with the required Kafka configurations and spark-kafka-Integreation.\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaPySparkIntegration\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\")\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### Step 5: Read Data from Kafka\n",
    "\n",
    "Configure the Kafka parameters and read the stream.\n",
    "\n",
    "```python\n",
    "kafka_bootstrap_servers = \"localhost:9092\"  # Update with your Kafka broker address\n",
    "kafka_topic = \"your_input_topic\"\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "  .option(\"subscribe\", kafka_topic) \\\n",
    "  .load()\n",
    "\n",
    "# Select the key and value columns and cast them to strings\n",
    "kafka_df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "```\n",
    "\n",
    "### Step 6: Process the Stream (Optional)\n",
    "\n",
    "You can perform various transformations on the DataFrame as per your requirement.\n",
    "\n",
    "```python\n",
    "processed_df = kafka_df.withColumn(\"uppercase_value\", col(\"value\").cast(StringType()))\n",
    "```\n",
    "\n",
    "### Step 7: Write Data to Kafka\n",
    "\n",
    "Configure the Kafka parameters and write the stream.\n",
    "\n",
    "```python\n",
    "output_kafka_topic = \"your_output_topic\"\n",
    "\n",
    "query = processed_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(uppercase_value AS STRING) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode('update')\\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", output_kafka_topic) \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### Complete Example\n",
    "\n",
    "Here is the complete example in one place:\n",
    "\n",
    "```python\n",
    "!pip install pyspark kafka-python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaPySparkIntegration\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Kafka configurations\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_input_topic\"\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "  .option(\"subscribe\", kafka_topic) \\\n",
    "  .load()\n",
    "\n",
    "# Process the data\n",
    "kafka_df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "processed_df = kafka_df.withColumn(\"uppercase_value\", col(\"value\").cast(StringType()))\n",
    "\n",
    "# Write to Kafka\n",
    "output_kafka_topic = \"your_output_topic\"\n",
    "\n",
    "query = processed_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(uppercase_value AS STRING) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode('update')\\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", output_kafka_topic) \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### Notes:\n",
    "\n",
    "1. Replace `\"localhost:9092\"`, `\"your_input_topic\"`, and `\"your_output_topic\"` with your actual Kafka server address and topic names.\n",
    "2. The `checkpointLocation` is necessary for Spark to maintain the state of the stream processing. Ensure the path is accessible and writeable.\n",
    "3. The transformations and processing steps are optional and can be customized based on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400212c",
   "metadata": {},
   "source": [
    "you need to start Zookeeper and Kafka brokers before creating the SparkSession and running your PySpark application. Kafka depends on Zookeeper for its operations, so Zookeeper must be running first. Here are the detailed steps:\n",
    "\n",
    "### Step 1: Start Zookeeper\n",
    "\n",
    "First, start the Zookeeper server. If you are using Kafka binaries, you can start Zookeeper with the following command:\n",
    "\n",
    "```bash\n",
    "# Navigate to your Kafka installation directory\n",
    "cd /path/to/kafka\n",
    "\n",
    "# Start Zookeeper\n",
    "bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "```\n",
    "\n",
    "### Step 2: Start Kafka Broker\n",
    "\n",
    "Once Zookeeper is running, start the Kafka broker:\n",
    "\n",
    "```bash\n",
    "# Navigate to your Kafka installation directory\n",
    "cd /path/to/kafka\n",
    "\n",
    "# Start Kafka broker\n",
    "bin/kafka-server-start.sh config/server.properties\n",
    "```\n",
    "\n",
    "### Step 3: Create Kafka Topics\n",
    "\n",
    "You need to create the input and output topics that your Spark application will use. You can create a Kafka topic using the following command:\n",
    "\n",
    "```bash\n",
    "# Create input topic\n",
    "bin/kafka-topics.sh --create --topic your_input_topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
    "\n",
    "# Create output topic\n",
    "bin/kafka-topics.sh --create --topic your_output_topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
    "```\n",
    "\n",
    "### Step 4: Verify Kafka Topics\n",
    "\n",
    "Ensure the topics are created correctly:\n",
    "\n",
    "```bash\n",
    "bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "```\n",
    "\n",
    "### Step 5: Start Jupyter Notebook and Create SparkSession\n",
    "\n",
    "With Zookeeper and Kafka running, you can now start your Jupyter Notebook and create the SparkSession.\n",
    "\n",
    "### Step 6: Follow the PySpark Kafka Integration Steps\n",
    "\n",
    "Follow the previously mentioned steps to read from and write to Kafka using PySpark in Jupyter Notebook:\n",
    "\n",
    "```python\n",
    "!pip install pyspark kafka-python\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaPySparkIntegration\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Kafka configurations\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_input_topic\"\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "  .option(\"subscribe\", kafka_topic) \\\n",
    "  .load()\n",
    "\n",
    "# Process the data\n",
    "kafka_df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "processed_df = kafka_df.withColumn(\"uppercase_value\", col(\"value\").cast(StringType()))\n",
    "\n",
    "# Write to Kafka\n",
    "output_kafka_topic = \"your_output_topic\"\n",
    "\n",
    "query = processed_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(uppercase_value AS STRING) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", output_kafka_topic) \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### Additional Notes\n",
    "\n",
    "- Ensure you have set the correct paths to your Kafka installation directory in the commands.\n",
    "- The `localhost:9092` address is used for local setups. For remote Kafka clusters, replace this with the appropriate address.\n",
    "- The `checkpointLocation` is necessary for stateful stream processing. Ensure this path is accessible and writable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bfc14",
   "metadata": {},
   "source": [
    "# Subscribe to 1 topic\n",
    "\n",
    "```python\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\")\n",
    "\n",
    "# Subscribe to multiple topics\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1,topic2\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to a pattern\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribePattern\", \"topic.*\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
