{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd37ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7070798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/01 08:00:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName('streaming-sink-source')\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6bf394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/01 09:52:29 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/07/01 09:52:29 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/07/01 09:52:32 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/07/01 09:52:32 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore xwyang@192.168.12.190\n",
      "24/07/01 09:52:38 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`default`.`flights_tble` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/07/01 09:52:38 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/07/01 09:52:39 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/07/01 09:52:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/07/01 09:52:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" create table if not exists flights_tble \n",
    "using csv options(path '/Users/xwyang/Desktop/data/flight_delays.csv')     \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b982ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default|  delays_tbl|      false|\n",
      "|  default|flights_tble|      false|\n",
      "|  default|    flys_tbl|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "562df5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('drop table delays_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2340764b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" create or replace temp view  flights_view\n",
    "using csv options(path '/Users/xwyang/Desktop/data/flight_delays.csv')     \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e402bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"select * from flights_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27f637f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|     _c0|  _c1|     _c2|   _c3|        _c4|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5541162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('/Users/xwyang/Desktop/data/flight_delays.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65f0ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|date   |delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|6    |602     |ABE   |ATL        |\n",
      "|1020600|-8   |369     |ABE   |DTW        |\n",
      "|1021245|-2   |602     |ABE   |ATL        |\n",
      "|1020605|-4   |602     |ABE   |ATL        |\n",
      "|1031245|-4   |602     |ABE   |ATL        |\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd55e359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.write.format('parquet').saveAsTable('fly_tble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2599a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1010630|  -10|     928|   RSW|        EWR|\n",
      "|1021029|   87|     974|   RSW|        ORD|\n",
      "|1021346|    0|     928|   RSW|        EWR|\n",
      "|1021044|   18|     928|   RSW|        EWR|\n",
      "|1021730|   29|     748|   RSW|        IAH|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('fly_tble').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0346cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('delays_tble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f81fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default|flights_tble|      false|\n",
      "|  default|    fly_tble|      false|\n",
      "|  default|    flys_tbl|      false|\n",
      "|         | delays_tble|       true|\n",
      "|         |flights_view|       true|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74fabb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/01 10:35:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/xs/5xq0dlqn045flbfc8gv1qn140000gn/T/temporary-bb3a16c2-3941-45bc-9b66-9e84f1a6317f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/07/01 10:35:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+----+\n",
      "|col0|col1|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import time\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    # Write a temporary text file to read it.\n",
    "    spark.createDataFrame([(1, \"2\"),]).write.mode(\"overwrite\").format(\"csv\").save(d)\n",
    "\n",
    "    # Start a streaming query to read the CSV file.\n",
    "    q = spark.readStream.schema(\n",
    "        \"col0 INT, col1 STRING\"\n",
    "    ).format(\"csv\").load(d).writeStream.format(\"console\").start()\n",
    "    time.sleep(3)\n",
    "    q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c40681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/01 10:42:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/xs/5xq0dlqn045flbfc8gv1qn140000gn/T/temporary-f71da5e1-1e03-4a74-b45a-fbdc98060ad6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/07/01 10:42:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|hello|\n",
      "| this|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import time\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    # Write a temporary text file to read it.\n",
    "    spark.createDataFrame(\n",
    "        [(\"hello\",), (\"this\",)]).write.mode(\"overwrite\").format(\"text\").save(d)\n",
    "\n",
    "    # Start a streaming query to read the text file.\n",
    "    q = spark.readStream.format(\"text\").load(d).writeStream.format(\"console\").start()\n",
    "    time.sleep(3)\n",
    "    q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "749652f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/01 10:44:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/07/01 10:44:47 WARN HiveMetaStore: Location: file:/Users/xwyang/spark-warehouse/new_tble specified for non-external table:new_tble\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"create table new_tble as select * from fly_tble   \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f580f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|date   |delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|3300630|-4   |473     |LGA   |CLT        |\n",
      "|3301200|-2   |160     |LGA   |BOS        |\n",
      "|3301400|-5   |160     |LGA   |BOS        |\n",
      "|3301600|46   |160     |LGA   |BOS        |\n",
      "|3301800|0    |160     |LGA   |BOS        |\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('new_tble').show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbe06b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "|Cathy| 28|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "da = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 28)]\n",
    "dff = spark.createDataFrame(da, [\"Name\", \"Age\"])\n",
    "dff.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9108251",
   "metadata": {},
   "source": [
    "Certainly! Here's the explanation and complete example in English:\n",
    "\n",
    "### Overview of Steps\n",
    "\n",
    "1. **Create SparkSession**: Initialize a SparkSession object.\n",
    "2. **Define Schema**: Define the schema for the CSV files.\n",
    "3. **Specify Directory Path**: Ensure the directory path is valid and contains CSV files.\n",
    "4. **Read Streaming CSV Files**: Use Structured Streaming to read CSV files from the directory.\n",
    "5. **Write Data to Console or Other Sink**: Write the streamed data to the console or another supported output sink.\n",
    "\n",
    "### Example Code\n",
    "\n",
    "Here is a complete example showing how to stream read CSV files from a directory and output the data to the console:\n",
    "\n",
    "```python\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Streaming CSV Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Specify CSV directory path\n",
    "csv_dir = \"/path/to/your/csv_directory\"\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(csv_dir):\n",
    "    raise FileNotFoundError(f\"The directory {csv_dir} does not exist.\")\n",
    "\n",
    "# Check if there are CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in the directory {csv_dir}.\")\n",
    "\n",
    "print(f\"CSV files found: {csv_files}\")\n",
    "\n",
    "# Read streaming CSV files\n",
    "df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .csv(csv_dir)\n",
    "\n",
    "# Write data to console\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to finish\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "1. **Create SparkSession**:\n",
    "   ```python\n",
    "   spark = SparkSession.builder \\\n",
    "       .appName(\"Streaming CSV Files\") \\\n",
    "       .getOrCreate()\n",
    "   ```\n",
    "   This initializes a SparkSession, which is the entry point for using Spark functionality.\n",
    "\n",
    "2. **Define Schema**:\n",
    "   ```python\n",
    "   schema = StructType([\n",
    "       StructField(\"Name\", StringType(), True),\n",
    "       StructField(\"Age\", IntegerType(), True),\n",
    "       StructField(\"Country\", StringType(), True)\n",
    "   ])\n",
    "   ```\n",
    "   Defines the schema of the CSV files, ensuring all files have the same structure.\n",
    "\n",
    "3. **Specify Directory Path and Check**:\n",
    "   ```python\n",
    "   csv_dir = \"/path/to/your/csv_directory\"\n",
    "\n",
    "   # Check if directory exists\n",
    "   if not os.path.exists(csv_dir):\n",
    "       raise FileNotFoundError(f\"The directory {csv_dir} does not exist.\")\n",
    "\n",
    "   # Check if there are CSV files in the directory\n",
    "   csv_files = [f for f in os.listdir(csv_dir) if f.endswith('.csv')]\n",
    "   if not csv_files:\n",
    "       raise FileNotFoundError(f\"No CSV files found in the directory {csv_dir}.\")\n",
    "\n",
    "   print(f\"CSV files found: {csv_files}\")\n",
    "   ```\n",
    "   Ensures the specified directory exists and contains CSV files. If not, raises an appropriate error.\n",
    "\n",
    "4. **Read Streaming CSV Files**:\n",
    "   ```python\n",
    "   df = spark.readStream \\\n",
    "       .schema(schema) \\\n",
    "       .csv(csv_dir)\n",
    "   ```\n",
    "   Uses the `readStream` method to read CSV files from the specified directory.\n",
    "\n",
    "5. **Write Data to Console**:\n",
    "   ```python\n",
    "   query = df.writeStream \\\n",
    "       .outputMode(\"append\") \\\n",
    "       .format(\"console\") \\\n",
    "       .start()\n",
    "\n",
    "   query.awaitTermination()\n",
    "   ```\n",
    "   Writes the streamed data to the console and starts the streaming query, then waits for it to terminate.\n",
    "\n",
    "By following these steps, you can use PySpark to stream read multiple CSV files from a directory, given that they all have the same schema, and process the data accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
