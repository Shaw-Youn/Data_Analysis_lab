{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56032b48",
   "metadata": {},
   "source": [
    "### Table deletes, updates, and merges\n",
    "\n",
    "Delta Lake supports several statements to facilitate deleting data from and updating data in Delta tables. Delta Lake is an open-source storage layer that brings ACID (Atomicity, Consistency, Isolation, Durability) transactions to Apache Spark and big data workloads.\n",
    "\n",
    "```\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (SparkSession.builder\\\n",
    "         .appName('delta-lake-transformation')\\\n",
    "         .config('spark.jars.packages','io.delta:delta-spark_2.12:3.2.0')\\\n",
    "         .config('spark.sql.extensions','io.delta.sql.DeltaSparkSessionExtension')\\\n",
    "         .config('spark.sql.catalog.spark_catalog','org.apache.spark.sql.delta.catalog.DeltaCatalog')\\\n",
    "         .config('spark.sql.catalogImplementation', 'hive')\\\n",
    "         .getOrCreate())\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144a984",
   "metadata": {},
   "source": [
    "### (A) Updating data to fix errors\n",
    "You can update data that matches a predicate in a Delta table. For example, in a table named people10m or a path at /tmp/delta/people-10m, to change an abbreviation in the gender column from M or F to Male or Female, you can run the following:\n",
    "```SQL\n",
    "UPDATE people10m SET gender = 'Female' WHERE gender = 'F';\n",
    "UPDATE people10m SET gender = 'Male' WHERE gender = 'M';\n",
    "\n",
    "UPDATE delta.`/tmp/delta/people-10m` SET gender = 'Female' WHERE gender = 'F';\n",
    "UPDATE delta.`/tmp/delta/people-10m` SET gender = 'Male' WHERE gender = 'M';\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/tmp/delta/people-10m')\n",
    "\n",
    "# Declare the predicate by using a SQL-formatted string.\n",
    "deltaTable.update(\n",
    "  condition = \"gender = 'F'\",\n",
    "  set = { \"gender\": \"'Female'\" }\n",
    ")\n",
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.update(\n",
    "  condition = col('gender') == 'M',\n",
    "  set = { 'gender': lit('Male') }\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f669fa",
   "metadata": {},
   "source": [
    "### (B) Deleting user-related data:\n",
    "You can remove data that matches a predicate from a Delta table. For instance, in a table named people10m or a path at /tmp/delta/people-10m, to delete all rows corresponding to people with a value in the birthDate column from before 1955, you can run the following:\n",
    "\n",
    "\n",
    "```SQL\n",
    "DELETE FROM people10m WHERE birthDate < '1955-01-01'\n",
    "\n",
    "DELETE FROM delta.`/tmp/delta/people-10m` WHERE birthDate < '1955-01-01'\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, '/tmp/delta/people-10m')\n",
    "\n",
    "# Declare the predicate by using a SQL-formatted string.\n",
    "deltaTable.delete(\"birthDate < '1955-01-01'\")\n",
    "\n",
    "# Declare the predicate by using Spark SQL functions.\n",
    "deltaTable.delete(col('birthDate') < '1960-01-01')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e8f5b",
   "metadata": {},
   "source": [
    "### (C) Upsert into a table using merge\n",
    "\n",
    "You can upsert data from a source table, view, or DataFrame into a target Delta table by using the MERGE SQL operation. Delta Lake supports inserts, updates and deletes in MERGE, and it supports extended syntax beyond the SQL standards to facilitate advanced use cases.\n",
    "\n",
    "Suppose you have a source table named people10mupdates or a source path at /tmp/delta/people-10m-updates that contains new data for a target table named people10m or a target path at /tmp/delta/people-10m. Some of these new records may already be present in the target data. To merge the new data, you want to update rows where the personâ€™s id is already present and insert the new rows where no matching id is present. You can run the following:\n",
    "\n",
    "```SQL\n",
    "MERGE INTO people10m\n",
    "USING people10mupdates\n",
    "ON people10m.id = people10mupdates.id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    id = people10mupdates.id,\n",
    "    firstName = people10mupdates.firstName,\n",
    "    middleName = people10mupdates.middleName,\n",
    "    lastName = people10mupdates.lastName,\n",
    "    gender = people10mupdates.gender,\n",
    "    birthDate = people10mupdates.birthDate,\n",
    "    ssn = people10mupdates.ssn,\n",
    "    salary = people10mupdates.salary\n",
    "WHEN NOT MATCHED\n",
    "  THEN INSERT (\n",
    "    id,\n",
    "    firstName,\n",
    "    middleName,\n",
    "    lastName,\n",
    "    gender,\n",
    "    birthDate,\n",
    "    ssn,\n",
    "    salary\n",
    "  )\n",
    "  VALUES (\n",
    "    people10mupdates.id,\n",
    "    people10mupdates.firstName,\n",
    "    people10mupdates.middleName,\n",
    "    people10mupdates.lastName,\n",
    "    people10mupdates.gender,\n",
    "    people10mupdates.birthDate,\n",
    "    people10mupdates.ssn,\n",
    "    people10mupdates.salary\n",
    "  )\n",
    "  ```\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "deltaTablePeople = DeltaTable.forPath(spark, '/tmp/delta/people-10m')\n",
    "deltaTablePeopleUpdates = DeltaTable.forPath(spark, '/tmp/delta/people-10m-updates')\n",
    "\n",
    "dfUpdates = deltaTablePeopleUpdates.toDF()\n",
    "\n",
    "deltaTablePeople.alias('people') \\\n",
    "  .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "    'people.id = updates.id'\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set =\n",
    "    {\n",
    "      \"id\": \"updates.id\",\n",
    "      \"firstName\": \"updates.firstName\",\n",
    "      \"middleName\": \"updates.middleName\",\n",
    "      \"lastName\": \"updates.lastName\",\n",
    "      \"gender\": \"updates.gender\",\n",
    "      \"birthDate\": \"updates.birthDate\",\n",
    "      \"ssn\": \"updates.ssn\",\n",
    "      \"salary\": \"updates.salary\"\n",
    "    }\n",
    "  ) \\\n",
    "  .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"id\": \"updates.id\",\n",
    "      \"firstName\": \"updates.firstName\",\n",
    "      \"middleName\": \"updates.middleName\",\n",
    "      \"lastName\": \"updates.lastName\",\n",
    "      \"gender\": \"updates.gender\",\n",
    "      \"birthDate\": \"updates.birthDate\",\n",
    "      \"ssn\": \"updates.ssn\",\n",
    "      \"salary\": \"updates.salary\"\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()\n",
    "```\n",
    "\n",
    "**Modify all unmatched rows using merge**\n",
    "\n",
    "Note\n",
    "\n",
    "WHEN NOT MATCHED BY SOURCE clauses are supported by the Scala, Python and Java Delta Lake APIs in Delta 2.3 and above. SQL is supported in Delta 2.4 and above.\n",
    "\n",
    "You can use the WHEN NOT MATCHED BY SOURCE clause to UPDATE or DELETE records in the target table that do not have corresponding records in the source table. We recommend adding an optional conditional clause to avoid fully rewriting the target table.\n",
    "\n",
    "The following code example shows the basic syntax of using this for deletes, overwriting the target table with the contents of the source table and deleting unmatched records in the target table.\n",
    "\n",
    "```python\n",
    "(targetDeltaTable\n",
    "  .merge(sourceDF, \"source.key = target.key\")\n",
    "  .whenMatchedUpdateAll()\n",
    "  .whenNotMatchedInsertAll()\n",
    "  .whenNotMatchedBySourceDelete()\n",
    "  .execute()\n",
    ")\n",
    "```\n",
    "\n",
    "In summary, this snippet performs an upsert operation on a Delta table, updating existing rows where the `id` matches and inserting new rows where there is no match. This is a common operation for keeping a Delta table in sync with a source DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52219c5c",
   "metadata": {},
   "source": [
    "In the context of a Delta Lake merge operation using PySpark, **the source is the DataFrame that contains the new data you want to merge into the existing Delta table, while the target is the existing Delta table where the merge operation will be applied.**\n",
    "\n",
    "Here is a summary of the roles:\n",
    "\n",
    "Target: The existing Delta table (ptbl) that you want to update or insert into.\n",
    "Source: The new data (btbl) represented as a DataFrame that you want to merge into the target Delta table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84972f37",
   "metadata": {},
   "source": [
    "### (D) Auditing Data changes with operation history\n",
    " \n",
    "```python\n",
    "deltapath = '/path/to/delta_table'\n",
    "deltaTable = DeltaTable.forPath(spark,deltapath) # load delta table\n",
    "\n",
    "deltaTable.history().columns\n",
    "['version','timestamp','userId','userName','operation','operationParameters', 'job','notebook','clusterId','readVersion','isolationLevel','isBlindAppend',\n",
    " 'operationMetrics','userMetadata','engineInfo']\n",
    "\n",
    "deltaTable.history().select('version','timestamp','operation','operationParameters').show(truncate=False)\n",
    "\n",
    "+-------+-----------------------+---------+------------------------------------------+\n",
    "|version|timestamp              |operation|operationParameters                       |\n",
    "+-------+-----------------------+---------+------------------------------------------+\n",
    "|3      |2024-07-24 23:34:20.892|UPDATE   |{predicate -> [\"(origin#8508 = LGA)\"]}    |\n",
    "|2      |2024-07-24 21:38:53.541|WRITE    |{mode -> Append, partitionBy -> []}       |\n",
    "|1      |2024-07-22 22:04:33.685|WRITE    |{mode -> Overwrite, partitionBy -> []}    |\n",
    "|0      |2024-07-22 18:04:28.729|WRITE    |{mode -> ErrorIfExists, partitionBy -> []}|\n",
    "+-------+-----------------------+---------+------------------------------------------+\n",
    "\n",
    "Note that the operatipn and operationParameters that are useful for auditing the changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89177358",
   "metadata": {},
   "source": [
    "### (E) Querying Previous snapshots of a table with time travel\n",
    "\n",
    "We can query previous versioned snapshots of a table by using **the DataFrameReader options \"versionAsOf\" and \"timestampAsOf\".**\n",
    "\n",
    "```python\n",
    "\n",
    "#In Python\n",
    "\n",
    "spark.read.format('delta').option(\"timestampAsOf\",\"2024-07-22\").load(delta_dir)\n",
    "spark.read.format('delta').option(\"versionAsOf\",\"2\").load(delta_dir)\n",
    "\n",
    "````\n",
    "\n",
    "This is useful in a variety of situations ,such as:\n",
    "\n",
    "- Reproducing machine learning experiments abd reperts by rerunning the job on a specfic table version.\n",
    "- Comparing the data changes between different versions for auditting.\n",
    "- Rolling back incorrect changes by reading a previous snpshots as a DataFrame and overwriting the table with it.\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
