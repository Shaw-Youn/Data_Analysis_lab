{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e04475",
   "metadata": {},
   "source": [
    "### This is a summary of how to read from and write to different Streaming data sources and sinks using  Spark. \n",
    "\n",
    "#### **1. Files:** CSV,JSON,Parquet,ORC,text,etc.\n",
    " \n",
    " csv_file_dir ='/path/to csv_file' # the directory of csv files with the same schema.\n",
    " output_dir = '/path/to/output_files' # the directory to save the files.\n",
    " check_dir = '/path/to/checkpoint_dir' # the directory of check point loaction.\n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    \n",
    "    spark = SparkSession.builder\\\n",
    "            .appName('files-stream-csv')\\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    df = spark.readStream.format('csv')\\\n",
    "              .schema(schema_file)\\    \n",
    "              .option('path',csv_file_dir)\\\n",
    "              .load()\n",
    "    \n",
    "    csv_stream_query = df.writeStream.format('csv')\\ \n",
    "                         .option('path',output_dir)\\\n",
    "                         .option('checkpointLocation',check_dir)\\\n",
    "                         .outputMode('append')\\  \n",
    "                         .start()\n",
    "    csv_stream_query.awaitTermination()\n",
    "    \n",
    "**Remarks:**\n",
    "\n",
    "     . All the files must be of the same format and are expected to have the same schema\n",
    "     . Structured Streaming supports writing streaming query output to files in the same formats as reads.\n",
    "     . For files, it only supports append mode.\n",
    "     . For CSV and JSON files, need to specify the schema.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc28980",
   "metadata": {},
   "source": [
    "**2.Apache Kafka:**  a popular publish/subscribe system. Apache Kafka is an open-source stream processing platform developed by the Apache Software Foundation. It is designed to handle real-time data feeds with high throughput and low latency. Here are some key features and components of Apache Kafka:\n",
    "\n",
    "#### Key Features:\n",
    "1. **Scalability**: Kafka is designed to scale horizontally, allowing you to add more brokers to handle increased data loads.\n",
    "2. **Fault Tolerance**: Data is replicated across multiple brokers, ensuring that even if one broker fails, the data remains available.\n",
    "3. **High Throughput**: Kafka can handle millions of messages per second with very low latency, making it suitable for high-throughput use cases.\n",
    "4. **Durability**: Messages are stored on disk and replicated across the cluster to prevent data loss.\n",
    "5. **Real-time Processing**: Kafka provides real-time stream processing capabilities, making it ideal for applications that require immediate data analysis.\n",
    "\n",
    "#### Core Components:\n",
    "1. **Broker**: Kafka cluster is made up of one or more servers, each called a broker. Brokers handle the storage and retrieval of messages.\n",
    "2. **Topic**: Messages in Kafka are categorized into topics. A topic is a logical channel to which producers send messages and from which consumers read messages.\n",
    "3. **Producer**: Producers are clients that send messages to Kafka topics. They can publish data to one or more topics.\n",
    "4. **Consumer**: Consumers read messages from Kafka topics. They can subscribe to one or more topics and process the incoming messages.\n",
    "5. **Partition**: Each topic is divided into partitions, which allow Kafka to parallelize the processing of messages. Partitions enable high scalability and fault tolerance.\n",
    "6. **ZooKeeper**: Kafka uses Apache ZooKeeper to manage and coordinate the Kafka brokers. ZooKeeper helps with leader election for partitions and maintaining configuration information.\n",
    "\n",
    "#### Use Cases:\n",
    "1. **Log Aggregation**: Kafka is commonly used to collect and aggregate log data from various systems for monitoring and analysis.\n",
    "2. **Real-time Analytics**: Companies use Kafka to process streaming data in real time for analytics and decision-making.\n",
    "3. **Data Integration**: Kafka serves as a data integration layer, allowing different systems to share and process data in real-time.\n",
    "4. **Event Sourcing**: Kafka can be used to store and process events in an event-driven architecture.\n",
    "\n",
    "#### How Kafka Works:\n",
    "- **Producers** send data to Kafka topics.\n",
    "- **Brokers** store the data in partitions within the topics.\n",
    "- **Consumers** subscribe to topics and read the data from the partitions.\n",
    "- Kafka ensures the data is replicated and distributed across multiple brokers to provide fault tolerance and high availability.\n",
    "\n",
    "Kafka's architecture and design make it a powerful tool for building robust, scalable, and real-time data processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd9112",
   "metadata": {},
   "source": [
    "**step 1:** Start zookeeper and kafka:\n",
    "\n",
    "     (a) .bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "     (b) .bin/kafka-server-start.sh config/server.properties\n",
    " \n",
    "**step 2:**  Create topics:\n",
    "     \n",
    "     (a) .bin/kafka-topics.sh --create --topic input-events --bootstrap-server localhost:9092\n",
    "         --replication-factor 3 --partitions 5\n",
    "     (b) .bin/kafka-topics.sh --create --topic output-events --bootstrap-server localhost:9092\n",
    "         --replication-factor 3 --partitions 5\n",
    "     (c) .bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "     \n",
    "**step 3:** producing/consuming message to/from the topic: \n",
    "\n",
    "      (a) .bin/kafka-console-producer.sh --topic input-events --bootstrap-server localhost:9092\n",
    "      (b) .bin/kafka-console-consumer.sh --topic output-events --from-beginning --bootstrap-server\n",
    "          localhost:9092\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    spark = SparkSession.builder\\\n",
    "            .appName('kafka-stream')\\\n",
    "            .config('spark.jars.packages','org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1')\\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    df = spark.readStream.format('kafka')\\\n",
    "              .option('kafka.bootstrap.servers','localhost:9092,host2:port2')\\    \n",
    "              .option('subscribe','input-events')\\\n",
    "              .load()\n",
    "    \n",
    "    query = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "              .writeStream \\\n",
    "              .format(\"kafka\") \\\n",
    "              .option(\"kafka.bootstrap.servers\", 'localhost:9092') \\\n",
    "              .option(\"topic\", output_kafka_topic) \\\n",
    "              .outputMode('update')\\\n",
    "              .option(\"checkpointLocation\", \"/path/to/checkpoint/dir\") \\\n",
    "              .start()\n",
    "\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38adec",
   "metadata": {},
   "source": [
    "**3. Writing to and reading from storage system: Cassandra**\n",
    "\n",
    "**step 1:**  Start cassandra and cqlsh:\n",
    "\n",
    "            (a) .bin/cassandra -f\n",
    "            (b) .bin/cqlsh\n",
    "            \n",
    "**step 2:** Create keysapce and table :\n",
    "\n",
    "       (a) cqlsh> create keyspace myspace with replication={'class':'SimpleStrategy','replication_factor:1};\n",
    "       (b) cqlsh> create table if not exists my_tbl(id int, name text,age int,nation text);\n",
    "       \n",
    "**step 3:** Start SparkSeesion \n",
    "\n",
    "\n",
    "       spark = SparkSession.builder\\\n",
    "                           .appName('spark-cassandra')\\\n",
    "                           .config('spark.jars.packages',\n",
    "                                   'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1')\\\n",
    "                           .config('spark.cassandra.connection.host','localhost')\\\n",
    "                           .config('spark.cassandra.connection.port',port_number)\\   (optional)\n",
    "                           .getOrCreate()\n",
    "                           \n",
    "                           \n",
    "       df = spark.read.format('org.apache.spark.sql.cassandra')\\\n",
    "                 .options(table='my_tbl',keyspace='myspace')\\\n",
    "                 .load()\n",
    "       \n",
    "       \n",
    "       df.write.format('org.apache.spark.sql.cassandra')\\\n",
    "               .options(table='my_tbl',keyspace='myspace')\\\n",
    "               .save()\n",
    "         \n",
    "**Remarks:**\n",
    "        \n",
    "        . table and keyspace exist before running sparksession app\n",
    "        . table must hhave the same schema as dataframe.\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
