{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfe1c9b",
   "metadata": {},
   "source": [
    "Here's an explanation of each of these metrics:\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "#### 1. **Confusion Matrix**\n",
    "A confusion matrix is a table used to describe the performance of a classification model on a set of test data for which the true values are known. It helps you understand the breakdown of predictions:\n",
    "\n",
    "- **True Positives (TP):** Correctly predicted positive observations.\n",
    "- **True Negatives (TN):** Correctly predicted negative observations.\n",
    "- **False Positives (FP):** Incorrectly predicted positive observations (Type I error).\n",
    "- **False Negatives (FN):** Incorrectly predicted negative observations (Type II error).\n",
    "\n",
    "The confusion matrix looks like this:\n",
    "```\n",
    "                Predicted Positive    Predicted Negative\n",
    "Actual Positive        TP                     FN\n",
    "Actual Negative        FP                     TN\n",
    "```\n",
    "\n",
    "#### 2. **Precision Score**\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. It answers the question: What proportion of positive identifications was actually correct?\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "#### 3. **Recall Score**\n",
    "Recall (or Sensitivity) is the ratio of correctly predicted positive observations to the all observations in actual class. It answers the question: What proportion of actual positives was identified correctly?\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "#### 4. **F1 Score**\n",
    "The F1 Score is the weighted average of Precision and Recall. It takes both false positives and false negatives into account. It is especially useful when the class distribution is imbalanced.\n",
    "\n",
    "$$ \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$\n",
    "\n",
    "#### 5. **Classification Report**\n",
    "A classification report provides a comprehensive overview of the precision, recall, F1 score, and support (the number of true instances for each label) for each class in a classification model. It is useful for understanding the detailed performance of the classifier.\n",
    "\n",
    "Example output of `classification_report`:\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.83      0.82      0.82       103\n",
    "           1       0.75      0.77      0.76        74\n",
    "\n",
    "    accuracy                           0.79       177\n",
    "   macro avg       0.79      0.79      0.79       177\n",
    "weighted avg       0.79      0.79      0.79       177\n",
    "```\n",
    "\n",
    "### Regression Metrics\n",
    "\n",
    "#### 6. **R² Score (Coefficient of Determination)**\n",
    "The R² score is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It ranges from 0 to 1, where:\n",
    "\n",
    "- **0:** The model does not explain any of the variance.\n",
    "- **1:** The model explains all the variance of the target variable.\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}$$\n",
    "\n",
    "Where $ y_i$ are the true values,$\\hat{y}_i$ are the predicted values, and $ \\bar{y}$ is the mean of true values.\n",
    "\n",
    "### Practical Example in Python\n",
    "Here's a practical example using the Iris dataset for classification and the Boston Housing dataset for regression:\n",
    "\n",
    "#### Classification Example\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "#### Regression Example\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load dataset\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R^2 Score:\", r2_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "These examples illustrate how to calculate and interpret the various metrics to assess the performance of classification and regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc5ecd",
   "metadata": {},
   "source": [
    "In statistical hypothesis testing, Type I and Type II errors are two types of errors that can occur when making decisions based on data. They are related to the incorrect rejection or acceptance of a null hypothesis (H₀).\n",
    "\n",
    "### Type I Error (False Positive)\n",
    "\n",
    "A Type I error occurs when the null hypothesis is true, but it is incorrectly rejected. In other words, it is the error of identifying an effect or difference when none actually exists. This is also known as a \"false positive.\"\n",
    "\n",
    "- **Example:** In the context of a medical test, a Type I error would occur if a test incorrectly indicates that a patient has a disease when they do not.\n",
    "- **Symbol:** The probability of making a Type I error is denoted by the Greek letter alpha (α), which is also known as the significance level of the test.\n",
    "\n",
    "### Type II Error (False Negative)\n",
    "\n",
    "A Type II error occurs when the null hypothesis is false, but it is incorrectly accepted. In other words, it is the error of failing to identify an effect or difference when one actually exists. This is also known as a \"false negative.\"\n",
    "\n",
    "- **Example:** In the context of a medical test, a Type II error would occur if a test incorrectly indicates that a patient does not have a disease when they actually do.\n",
    "- **Symbol:** The probability of making a Type II error is denoted by the Greek letter beta (β).\n",
    "\n",
    "### Relationship Between Type I and Type II Errors\n",
    "\n",
    "There is often a trade-off between Type I and Type II errors. Reducing the probability of one type of error typically increases the probability of the other type of error.\n",
    "\n",
    "- **Significance Level (α):** Lowering the significance level (making the test more stringent) decreases the probability of a Type I error but may increase the probability of a Type II error.\n",
    "- **Power of the Test (1 - β):** The power of a test is the probability that it correctly rejects a false null hypothesis (i.e., it detects an effect when there is one). Increasing the power of a test decreases the probability of a Type II error.\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "Here is a visual representation to illustrate the concept:\n",
    "\n",
    " |                | H₀ True         | H₀ False          |\n",
    "|----------------|-----------------|-------------------|\n",
    "| **Reject H₀**  | Type I Error    | Correct Decision  |\n",
    "| **Fail to Reject H₀** | Correct Decision  | Type II Error    |\n",
    "  \n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Type I Error (False Positive):** Incorrectly rejecting a true null hypothesis (finding an effect that is not there). Represented by α.\n",
    "- **Type II Error (False Negative):** Incorrectly accepting a false null hypothesis (not finding an effect that is there). Represented by β.\n",
    "\n",
    "Understanding these errors is crucial in designing experiments and interpreting statistical tests, as they impact the reliability and validity of the conclusions drawn from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
